import argparse
from datetime import datetime
import json
import os
import subprocess
import time
import numpy as np
import signal
import multiprocessing as mp
from pathlib import Path
import sys

# ë¶„ì„ ëª¨ë“ˆ import ì¶”ê°€
from exp3_analysis import EXP3MultiSeedAnalyzer


def signal_handler(signum, frame):
    """
    Signal handler function that is called when a signal is received.

    Args:
        signum (int): The signal number.
        frame (frame): The current stack frame.

    Returns:
        None
    """
    print(f"Received signal {signum}, exiting...")
    remove_temp_json(json_file_list)
    exit(1)


def generate_config_dict_list(config_file, execution_timestamp=None):
    """
    Generate a list of dictionaries containing different configurations based on the input JSON file.

    Args:
        config_file (str): The path to the JSON file containing the configuration.
        execution_timestamp (str): The execution timestamp to use for all configs (YYYYMMDD_HHMMSS format)

    Returns:
        list: A list of dictionaries, where each dictionary represents a different configuration.
    """
    # Load the contents of the JSON file into a dictionary
    with open(config_file) as f:
        config = json.load(f)

    seed_max = config['seed']
    power_dBm = config['power_dBm']
    power_dBm_end = config['power_dBm_end']
    power_dBm_step = config['power_dBm_step']

    # Define the range of power values to test
    if power_dBm == power_dBm_end:
        power_values = [power_dBm]
    # If the finish power value is `-inf` then the range of power values will be from the start power value to `0dBm` in steps of `power_dBm_step`, and then an additional value of `-np.inf` will be added to the end of the list
    elif power_dBm_end == "-np.inf":
        power_values = np.arange(power_dBm, 0, -power_dBm_step)
        power_values = np.append(power_values, -np.inf)
    else:
        power_values = np.arange(power_dBm, power_dBm_end-power_dBm_step, -power_dBm_step)

    # Seed values to test 
    seeds = list(range(seed_max))

    # Define the number range of variable power cells
    n_variable_power_cells = list(range(1, config['n_variable_power_cells']+1))

    # Create a list of dictionaries
    config_dict_list = []
    for seed in seeds:
        for power_dBm in power_values:
            if len(n_variable_power_cells) == 0:
                # Create a new dictionary object for each iteration
                config_copy = config.copy()
                
                # Update the seed and power_dBm value in the new dictionary object
                config_copy['seed'] = seed
                config_copy['variable_cell_power_dBm'] = power_dBm
                config_copy['n_variable_power_cells'] = 0
                
                # Add execution timestamp if provided
                if execution_timestamp:
                    config_copy['execution_timestamp'] = execution_timestamp
                
                # Append the new dictionary object to the list
                config_dict_list.append(config_copy)
            else:
                # Create a new dictionary object for each iteration
                for n_cells in n_variable_power_cells:
                    config_copy = config.copy()
                    
                    # Update the values in the new dictionary object
                    config_copy['seed'] = seed
                    config_copy['variable_cell_power_dBm'] = power_dBm
                    config_copy['n_variable_power_cells'] = n_cells
                    
                    # Add execution timestamp if provided
                    if execution_timestamp:
                        config_copy['execution_timestamp'] = execution_timestamp
                    
                    # Append the new dictionary object to the list
                    config_dict_list.append(config_copy)

    return config_dict_list


def create_updated_json_file(config_dict, output_file):
    """
    Create an updated JSON file with the given configuration dictionary.

    Args:
        config_dict (dict): The configuration dictionary to write to the JSON file.
        output_file (str): The path to the output JSON file.

    Returns:
        None
    """
    # Write the updated dictionary to the output JSON file
    with open(output_file, 'w') as f:
        json.dump(config_dict, f, indent=4)


# Define a function that takes a list of dictionaries and dumps each dictionary to a separate JSON file
def dump_json(config_dict_list):
    # Create a list to store the JSON file paths
    json_file_list = []

    # Loop through each dictionary in the list
    for i, config_dict in enumerate(config_dict_list):
        # Create a unique JSON file name
        json_file_name = f"temp_{i}.json"

        # Dump the dictionary to the JSON file
        with open(json_file_name, 'w') as f:
            json.dump(config_dict, f, indent=4)

        # Append the JSON file path to the list
        json_file_list.append(json_file_name)

    # Return the list of JSON file paths
    return json_file_list


# Define a function that takes a single json file and runs kiss.py against it
def run_kiss(temp_json_file):
    # Define the command to execute
    command = ["python", "kiss.py", "-c", f"{temp_json_file}"]

    # Execute the command as a subprocess
    process = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    # Print the output of the subprocess
    print(process.stdout.decode())
    if process.returncode != 0:
        print(process.stderr.decode())
        return process.returncode
    else:
        return process.returncode


# Define a function that takes a list of temp JSON file paths and removes them from the file system
def remove_temp_json(json_file_list):
    # Loop through each JSON file in the list
    for json_file in json_file_list:
        # Remove the JSON file from the file system
        try:
            os.remove(json_file)
            # test if file exists
            if os.path.exists(json_file):
                # tried removing and it still exists, so raise an error by moving to the except block below
                raise OSError
            else:
                # Get the json file name
                json_file_name = os.path.basename(json_file)
                print(f"File {json_file_name} removed successfully")
        except OSError as e:
            print(f"Error deleting file: {e}")


# Callback function to update progress
def update_progress(result):
    with progress_counter.get_lock():
        progress_counter.value += 1
        print(f"Processed {progress_counter.value} of {total_files} files")


# Global variables for signal handler
json_file_list = []
progress_counter = None
total_files = 0


def run_analysis(config_file, output_base_dir="_/data/output", current_run_timestamp=None):
    """
    ì‹¤í—˜ ì™„ë£Œ í›„ ìë™ ë¶„ì„ ì‹¤í–‰
    
    Args:
        config_file (str): ì‚¬ìš©ëœ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        output_base_dir (str): ì¶œë ¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬
        current_run_timestamp (str): í˜„ì¬ ì‹¤í–‰ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ (YYYY_MM_DD/HHMMSS í˜•ì‹)
    """
    print("\n" + "="*60)
    print("ğŸ”¬ EXP3 ì‹¤í—˜ ê²°ê³¼ ìë™ ë¶„ì„ ì‹œì‘...")
    print("="*60)
    
    # ì„¤ì • íŒŒì¼ì—ì„œ ì‹¤í—˜ ì´ë¦„ ì¶”ì¶œ
    with open(config_file, 'r') as f:
        config = json.load(f)
    
    experiment_name = config.get('experiment_description', 'exp3_experiment')
    
    # í˜„ì¬ ì‹¤í–‰ì˜ ê²°ê³¼ ë””ë ‰í† ë¦¬ ì§€ì •
    if current_run_timestamp:
        # í˜„ì¬ ì‹¤í–‰ì˜ ì •í™•í•œ í´ë” ì§€ì •
        search_dir = Path(output_base_dir) / experiment_name / current_run_timestamp
        print(f"ğŸ“ í˜„ì¬ ì‹¤í–‰ ê²°ê³¼ ë””ë ‰í† ë¦¬: {search_dir}")
        
        # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
        if not search_dir.exists():
            print(f"âš ï¸ ê²½ê³ : ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {search_dir}")
            print("ğŸ“‚ í˜„ì¬ ì¡´ì¬í•˜ëŠ” ë””ë ‰í† ë¦¬ í™•ì¸ ì¤‘...")
            
            # ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
            parent_dir = Path(output_base_dir) / experiment_name
            if parent_dir.exists():
                subdirs = [d for d in parent_dir.iterdir() if d.is_dir()]
                print(f"   ë°œê²¬ëœ ë””ë ‰í† ë¦¬: {[d.name for d in subdirs[-5:]]}")  # ìµœê·¼ 5ê°œë§Œ í‘œì‹œ
            
            # ëŒ€ì²´ ê²½ë¡œ ì‹œë„ (data í´ë”)
            alt_search_dir = Path("data/output") / experiment_name / current_run_timestamp.replace("_/", "")
            if alt_search_dir.exists():
                print(f"âœ… ëŒ€ì²´ ê²½ë¡œì—ì„œ ë°œê²¬: {alt_search_dir}")
                search_dir = alt_search_dir
            else:
                print("âŒ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ ê°™ì€ ë””ë ‰í† ë¦¬ì— ì €ì¥
        analysis_output_dir = search_dir / "analysis_results"
    else:
        search_dir = Path(output_base_dir)
        # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìœ¼ë©´ ë³„ë„ ë””ë ‰í† ë¦¬ì— ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        analysis_output_dir = Path(f"exp3_analysis_{experiment_name}_{timestamp}")
    
    # ë¶„ì„ê¸° ì‹¤í–‰
    analyzer = EXP3MultiSeedAnalyzer(
        results_dir=search_dir,
        config_file=config_file,
        output_dir=analysis_output_dir
    )
    
    try:
        analyzer.run()
        print(f"\nâœ… ë¶„ì„ ê²°ê³¼ê°€ '{analysis_output_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì£¼ìš” ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        summary_file = analysis_output_dir / 'analysis_summary.txt'
        if summary_file.exists():
            print("\nğŸ“‹ ë¶„ì„ ìš”ì•½:")
            print("-" * 40)
            with open(summary_file, 'r') as f:
                print(f.read())
                
    except Exception as e:
        print(f"\nâŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    # Register the signal handler function to handle the SIGINT and SIGTERM signals
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # Parse the command line arguments
    parser = argparse.ArgumentParser(
        description='Run kiss.py against a specified config value.')
    parser.add_argument(
        '-c',
        '--config-file',
        type=str,
        required=True,
        default='_test/data/input/kiss/test_kiss.json'
    )
    parser.add_argument(
        '--no-analysis',
        action='store_true',
        help='ë¶„ì„ì„ ê±´ë„ˆë›°ê³  ì‹œë®¬ë ˆì´ì…˜ë§Œ ì‹¤í–‰'
    )
    parser.add_argument(
        '--analysis-only',
        action='store_true',
        help='ì‹œë®¬ë ˆì´ì…˜ì„ ê±´ë„ˆë›°ê³  ê¸°ì¡´ ê²°ê³¼ë§Œ ë¶„ì„'
    )
    parser.add_argument(
        '--analysis-date',
        type=str,
        help='ë¶„ì„í•  ì‹¤í–‰ ë‚ ì§œ (YYYY_MM_DD í˜•ì‹, --analysis-onlyì™€ í•¨ê»˜ ì‚¬ìš©)'
    )
    
    args = parser.parse_args()

    # ë¶„ì„ë§Œ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°
    if args.analysis_only:
        run_analysis(args.config_file, current_run_timestamp=args.analysis_date)
        exit(0)

    # Start the timer
    start_time = time.time()
    
    # í˜„ì¬ ì‹¤í–‰ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ëª¨ë“  ì‹œë“œê°€ ê³µìœ )
    current_run_date = datetime.now().strftime("%Y_%m_%d")
    current_run_time = datetime.now().strftime("%H%M%S")
    execution_timestamp = f"{current_run_date}_{current_run_time}"

    print(f"ğŸš€ Starting execution with timestamp: {execution_timestamp}")

    config_dict_list = generate_config_dict_list(args.config_file, execution_timestamp=execution_timestamp)

    # Dump the list of dictionaries to individual JSON files
    json_file_list = dump_json(config_dict_list)
    total_files = len(json_file_list)

    # Initialize a multiprocessing pool with more processes for your 14-core CPU
    # Using 12 processes to leave some CPU headroom for the system
    num_processes = 12
    num_tasks_per_child = 1
    progress_counter = mp.Value('i', 0)  # Shared variable to keep track of progress

    print(f"Starting parallel processing with {num_processes} processes for {total_files} simulations...")
    print(f"Results will be saved in: data/output/<experiment>/{current_run_date}/{current_run_time}/")

    with mp.Pool(processes=num_processes, maxtasksperchild=num_tasks_per_child) as pool:
        # Store async results
        async_results = []
        
        # Submit all tasks asynchronously
        for json_file in json_file_list:
            # Apply async with callback for progress tracking
            result = pool.apply_async(run_kiss, args=(json_file,), callback=update_progress)
            async_results.append(result)
        
        # Close the pool to prevent any more tasks from being submitted
        pool.close()
        
        # Wait for all processes to complete
        pool.join()
        
        # Optionally, check for any errors in the results
        for i, result in enumerate(async_results):
            try:
                return_code = result.get(timeout=1)  # Should already be done
                if return_code != 0:
                    print(f"Warning: Task {i+1} returned non-zero exit code: {return_code}")
            except Exception as e:
                print(f"Error in task {i+1}: {e}")

    # Remove the temporary JSON files
    remove_temp_json(json_file_list)

    # Stop the timer
    end_time = time.time()

    # Calculate the elapsed time
    elapsed_time = end_time - start_time

    # Print the elapsed time
    print(f"All subprocesses completed in {elapsed_time:.2f} seconds")
    print(f"Average time per simulation: {elapsed_time/total_files:.2f} seconds")
    
    # ìë™ ë¶„ì„ ì‹¤í–‰ (--no-analysis ì˜µì…˜ì´ ì—†ëŠ” ê²½ìš°)
    if not args.no_analysis:
        # ì ì‹œ ëŒ€ê¸° (íŒŒì¼ ì‹œìŠ¤í…œ ë™ê¸°í™”)
        time.sleep(2)
        
        # ë¶„ì„ ì‹¤í–‰ - ì „ì²´ íƒ€ì„ìŠ¤íƒ¬í”„ ì „ë‹¬
        run_analysis(args.config_file, current_run_timestamp=f"{current_run_date}/{current_run_time}")
    else:
        print("\nğŸ’¡ ë¶„ì„ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë¶„ì„í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:")
        print(f"   python run_kiss.py -c {args.config_file} --analysis-only --analysis-date {current_run_date}/{current_run_time}")