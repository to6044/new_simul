#!/usr/bin/env python3
"""
EXP3 Multi-seed Evaluation Script (ë³‘ë ¬ ì‹¤í–‰ ë²„ì „)
"""

import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
import subprocess
import time
from typing import Dict, List, Tuple
import scipy.stats as stats
import pandas as pd
from multiprocessing import Pool, cpu_count
import concurrent.futures
import glob

class EXP3MultiSeedEvaluatorParallel:
    def __init__(self, base_config_path: str, n_seeds: int = 10, output_base_dir: str = "_/data/output", n_parallel: int = None):
        """
        Parameters:
        -----------
        base_config_path : str
            ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        n_seeds : int
            ì‹¤í—˜í•  ì‹œë“œ ê°œìˆ˜
        output_base_dir : str
            ê²°ê³¼ ì €ì¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬
        n_parallel : int
            ë³‘ë ¬ ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜ - 1)
        """
        self.base_config_path = base_config_path
        self.n_seeds = n_seeds
        self.output_base_dir = output_base_dir
        self.n_parallel = n_parallel or max(1, cpu_count() - 1)
        
        # ì‹œê°„ ê¸°ë°˜ í´ë” ìƒì„±
        now = datetime.now()
        date_str = now.strftime("%Y%m%d")   
        time_str = now.strftime("%H%M%S") 
        date_dir = os.path.join(output_base_dir, f"exp3_multi_seed_{date_str}")
        os.makedirs(date_dir, exist_ok=True)
        self.experiment_dir = os.path.join(date_dir, time_str)
        os.makedirs(self.experiment_dir, exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜
        self.all_results = []
        self.seed_results = {}
        
    def run_single_seed_wrapper(self, args):
        """ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
        seed, config_dict = args
        return self.run_single_seed(seed, config_dict)
        
    def run_single_seed(self, seed: int, config_dict: dict = None) -> Dict:
        """ë‹¨ì¼ ì‹œë“œë¡œ ì‹¤í—˜ ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"[Seed {seed}] Starting experiment")
        print(f"{'='*60}")
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
        if config_dict is None:
            with open(self.base_config_path, 'r') as f:
                config_dict = json.load(f)
        
        # ì‹œë“œ ë° ì¶œë ¥ ê²½ë¡œ ìˆ˜ì •
        config_dict['seed'] = seed
        config_dict['exp3_learning_log'] = f"exp3_learning_seed_{seed}.json"
        config_dict['exp3_final_model'] = f"exp3_model_seed_{seed}.json"
        
        # ê° ì‹œë“œë³„ ê°œë³„ í´ë” ìƒì„±
        seed_output_dir = os.path.join(self.experiment_dir, f"seed_{seed}")
        os.makedirs(seed_output_dir, exist_ok=True)
        
        # ì„ì‹œ ì„¤ì • íŒŒì¼ ìƒì„±
        temp_config_path = os.path.join(seed_output_dir, f"config_seed_{seed}.json")
        with open(temp_config_path, 'w') as f:
            json.dump(config_dict, f, indent=2)
        
        # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        start_time = time.time()
        process = subprocess.run(
            ["python", "run_kiss.py", "-c", temp_config_path],
            capture_output=True,
            text=True,
            cwd=os.getcwd()  # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ëª…ì‹œ
        )
        
        if process.returncode != 0:
            print(f"[Seed {seed}] Error: {process.stderr}")
            return None
            
        execution_time = time.time() - start_time
        print(f"[Seed {seed}] Completed in {execution_time:.2f} seconds")
        
        # ê²°ê³¼ íŒŒì¼ ì°¾ê¸° - ë” ìœ ì—°í•œ ë°©ë²•
        result_files = self._find_result_files(config_dict, seed)
        
        if not result_files['learning_log']:
            print(f"[Seed {seed}] Warning: Learning log not found, searching alternative locations...")
            result_files = self._find_result_files_alternative(config_dict, seed)
        
        if not result_files['learning_log']:
            print(f"[Seed {seed}] Error: Could not find learning log file")
            return None
        
        # ê²°ê³¼ ë¡œë“œ
        try:
            with open(result_files['learning_log'], 'r') as f:
                learning_data = json.load(f)
                
            model_data = {}
            if result_files['model'] and os.path.exists(result_files['model']):
                with open(result_files['model'], 'r') as f:
                    model_data = json.load(f)
                    
            print(f"[Seed {seed}] Successfully loaded results")
            
            return {
                'seed': seed,
                'learning_data': learning_data,
                'model_data': model_data,
                'execution_time': execution_time,
                'result_files': result_files
            }
            
        except Exception as e:
            print(f"[Seed {seed}] Error loading results: {e}")
            return None
    
    def _find_result_files(self, config: dict, seed: int) -> dict:
        """ê²°ê³¼ íŒŒì¼ ì°¾ê¸° (ê¸°ë³¸ ë°©ë²•)"""
        exp_desc = config.get('experiment_description')
        base_path = os.path.join(self.output_base_dir, exp_desc)
        
        learning_log_name = config.get('exp3_learning_log', f'exp3_learning_seed_{seed}.json')
        model_name = config.get('exp3_final_model', f'exp3_model_seed_{seed}.json')
        
        result_files = {
            'learning_log': None,
            'model': None
        }
        
        # ë‚ ì§œ/ì‹œê°„ í´ë” êµ¬ì¡° íƒìƒ‰
        if os.path.exists(base_path):
            # ê°€ì¥ ìµœê·¼ ë‚ ì§œ í´ë” ì°¾ê¸°
            date_dirs = sorted([d for d in os.listdir(base_path) 
                              if os.path.isdir(os.path.join(base_path, d))])
            
            if date_dirs:
                latest_date = date_dirs[-1]
                date_path = os.path.join(base_path, latest_date)
                
                # ì‹œê°„ í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
                time_dirs = sorted([d for d in os.listdir(date_path) 
                                  if os.path.isdir(os.path.join(date_path, d))])
                
                if time_dirs:
                    # ê°€ì¥ ìµœê·¼ ì‹œê°„ í´ë”
                    search_path = os.path.join(date_path, time_dirs[-1])
                else:
                    # ë‚ ì§œ í´ë” ì§ì ‘ ì‚¬ìš©
                    search_path = date_path
                
                # íŒŒì¼ ì°¾ê¸°
                learning_log_path = os.path.join(search_path, learning_log_name)
                model_path = os.path.join(search_path, model_name)
                
                if os.path.exists(learning_log_path):
                    result_files['learning_log'] = learning_log_path
                if os.path.exists(model_path):
                    result_files['model'] = model_path
        
        return result_files
    
    def _find_result_files_alternative(self, config: dict, seed: int) -> dict:
        """ê²°ê³¼ íŒŒì¼ ì°¾ê¸° (ëŒ€ì²´ ë°©ë²•)"""
        result_files = {
            'learning_log': None,
            'model': None
        }
        
        # ì „ì²´ ì¶œë ¥ ë””ë ‰í† ë¦¬ì—ì„œ ê²€ìƒ‰
        search_patterns = [
            f"**/exp3_learning*seed_{seed}.json",
            f"**/exp3_learning_seed_{seed}.json",
            f"**/exp3_learning_progress*seed_{seed}.json",
            f"**/*seed_{seed}*learning*.json"
        ]
        
        for pattern in search_patterns:
            files = glob.glob(os.path.join(self.output_base_dir, pattern), recursive=True)
            if files:
                # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
                files.sort(key=os.path.getmtime)
                result_files['learning_log'] = files[-1]
                print(f"[Seed {seed}] Found learning log: {result_files['learning_log']}")
                break
        
        # ëª¨ë¸ íŒŒì¼ë„ ë¹„ìŠ·í•˜ê²Œ ê²€ìƒ‰
        if result_files['learning_log']:
            model_patterns = [
                f"**/exp3_model*seed_{seed}.json",
                f"**/exp3_trained*seed_{seed}.json",
                f"**/*seed_{seed}*model*.json"
            ]
            
            for pattern in model_patterns:
                files = glob.glob(os.path.join(self.output_base_dir, pattern), recursive=True)
                if files:
                    files.sort(key=os.path.getmtime)
                    result_files['model'] = files[-1]
                    break
        
        return result_files
    
    def run_all_seeds_parallel(self):
        """ëª¨ë“  ì‹œë“œë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
        print(f"\n{'='*80}")
        print(f"Starting parallel multi-seed evaluation")
        print(f"Seeds: {self.n_seeds}, Parallel processes: {self.n_parallel}")
        print(f"Output directory: {self.experiment_dir}")
        print(f"{'='*80}")
        
        # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
        with open(self.base_config_path, 'r') as f:
            base_config = json.load(f)
        
        # ê° ì‹œë“œì— ëŒ€í•œ ì„¤ì • ì¤€ë¹„
        seed_configs = [(seed, base_config.copy()) for seed in range(1, self.n_seeds + 1)]
        
        # ë³‘ë ¬ ì‹¤í–‰
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.n_parallel) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_seed = {
                executor.submit(self.run_single_seed, seed, config): seed 
                for seed, config in seed_configs
            }
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in concurrent.futures.as_completed(future_to_seed):
                seed = future_to_seed[future]
                try:
                    result = future.result()
                    if result:
                        self.all_results.append(result)
                        self.seed_results[seed] = result
                        print(f"[Seed {seed}] âœ… Successfully processed")
                    else:
                        print(f"[Seed {seed}] âŒ Failed to process")
                except Exception as e:
                    print(f"[Seed {seed}] âŒ Exception: {e}")
        
        print(f"\n{'='*80}")
        print(f"Completed {len(self.all_results)}/{self.n_seeds} experiments successfully")
        print(f"{'='*80}")
    
    def calculate_metrics(self):
        """í‰ê°€ ì§€í‘œ ê³„ì‚° (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)"""
        if not self.all_results:
            print("No results to analyze")
            return None
            
        metrics = {
            'cumulative_rewards': [],
            'cumulative_regrets': [],
            'average_regrets': [],
            'final_efficiencies': [],
            'convergence_episodes': [],
            'energy_savings': [],
            'average_throughputs': [],
            'switching_costs': [],
            'best_arm_selections': []
        }
        
        for result in self.all_results:
            learning_data = result['learning_data']
            model_data = result['model_data']
            
            # ëˆ„ì  ë³´ìƒ
            rewards = learning_data.get('reward_history', [])
            if rewards:
                cumulative_reward = np.cumsum(rewards)
                metrics['cumulative_rewards'].append(cumulative_reward)
            
            # ì—ë„ˆì§€ í†µê³„
            energy_stats = learning_data.get('energy_statistics', {})
            if energy_stats:
                energy_saving = energy_stats.get('avg_energy_saving_all_on', 0)
                metrics['energy_savings'].append(energy_saving)
            
            # Throughput í†µê³„
            throughput_stats = learning_data.get('throughput_statistics', {})
            if throughput_stats:
                avg_throughput = throughput_stats.get('avg_cell_throughput_gbps', 0)
                metrics['average_throughputs'].append(avg_throughput)
            
            # ì „í™˜ ë¹„ìš©
            switching_rate = learning_data.get('switching_rate', 0)
            metrics['switching_costs'].append(switching_rate * 100)
            
            # ìˆ˜ë ´ ì—í”¼ì†Œë“œ
            convergence_episode = learning_data.get('convergence_episode', -1)
            metrics['convergence_episodes'].append(convergence_episode)
            
            # Regret ì •ë³´
            regret_stats = learning_data.get('regret_statistics', {})
            if regret_stats:
                metrics['cumulative_regrets'].append(regret_stats.get('cumulative_regret', 0))
                metrics['average_regrets'].append(regret_stats.get('average_regret', 0))
            
            # íš¨ìœ¨ì„±
            max_efficiency = learning_data.get('max_efficiency', 0)
            metrics['final_efficiencies'].append(max_efficiency)
            
            # ìµœì  arm
            if model_data and 'weights' in model_data:
                weights = np.array(model_data['weights'])
                best_arm = np.argmax(weights) if len(weights) > 0 else -1
                metrics['best_arm_selections'].append(best_arm)
        
        return metrics
    
    # ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ (generate_plots, save_summary_report ë“±)ì€ ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼
    def generate_plots(self, metrics: Dict):
        """í‰ê°€ ê²°ê³¼ í”Œë¡¯ ìƒì„±"""
        # ê¸°ì¡´ EXP3MultiSeedEvaluatorì˜ generate_plots ë©”ì„œë“œì™€ ë™ì¼
        # (ì½”ë“œ ê¸¸ì´ ê´€ê³„ë¡œ ìƒëµ - ê¸°ì¡´ ì½”ë“œ ë³µì‚¬)
        pass
    
    def save_summary_report(self, metrics: Dict):
        """ì¢…í•© ë³´ê³ ì„œ ì €ì¥"""
        # ê¸°ì¡´ EXP3MultiSeedEvaluatorì˜ save_summary_report ë©”ì„œë“œì™€ ë™ì¼
        # (ì½”ë“œ ê¸¸ì´ ê´€ê³„ë¡œ ìƒëµ - ê¸°ì¡´ ì½”ë“œ ë³µì‚¬)
        pass


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='EXP3 Multi-seed Parallel Evaluation')
    parser.add_argument('-c', '--config', type=str, required=True,
                      help='Base configuration file path')
    parser.add_argument('-n', '--n-seeds', type=int, default=10,
                      help='Number of seeds to run (default: 10)')
    parser.add_argument('-p', '--n-parallel', type=int, default=None,
                      help='Number of parallel processes (default: CPU count - 1)')
    parser.add_argument('-o', '--output-dir', type=str, default='_/data/output',
                      help='Output directory (default: _/data/output)')
    parser.add_argument('--debug', action='store_true',
                      help='Enable debug mode (run only 1 seed)')
    
    args = parser.parse_args()
    
    if args.debug:
        print("ğŸ› Debug mode: Running only 1 seed")
        args.n_seeds = 1
    
    # í‰ê°€ê¸° ìƒì„± ë° ì‹¤í–‰
    evaluator = EXP3MultiSeedEvaluatorParallel(
        base_config_path=args.config,
        n_seeds=args.n_seeds,
        output_base_dir=args.output_dir,
        n_parallel=args.n_parallel
    )
    
    # ë³‘ë ¬ë¡œ ëª¨ë“  ì‹œë“œ ì‹¤í–‰
    evaluator.run_all_seeds_parallel()
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = evaluator.calculate_metrics()
    
    if metrics:
        # í”Œë¡¯ ìƒì„±
        evaluator.generate_plots(metrics)
        
        # ë³´ê³ ì„œ ì €ì¥
        evaluator.save_summary_report(metrics)
        
        print(f"\n{'='*80}")
        print(f"âœ… Evaluation complete!")
        print(f"ğŸ“ Results saved in: {evaluator.experiment_dir}")
        print(f"{'='*80}")
    else:
        print("âŒ No metrics to analyze. Please check the experiment results.")


if __name__ == "__main__":
    main()
